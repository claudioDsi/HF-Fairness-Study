repo_name,keypoints,Code,bias,objectionable,hallucination,mitigation
01-ai/Yi-6B,"1. The model have high diversity response, but will amplify issues including hallucination, inconsistent output.
2. The model supports commercial use, only after complete the form.
3. Authors use data compliance algorithm during training to ensure the model compliance.
4.THe data is complex, and language model is diverse, the model still can generate incorrect output.
5. The authors are not responsible for risks from model misuse, as well as associated data security concerns.","C1, C4, C19",,,brief mention,
1-800-BAD-CODE/punctuation_fullstop_truecase_english,"1. Training data in year 2012 was used to reduce bias, as data in 2021 is dominated by COVID discussion.",C127,,,,
51la5/roberta-large-NER,"1. Users both downstream and direct should be made aware of the risks, biases, and limitations of the model.
2. Make reference to literatures on biases and fairness issues.
3. Make an code example of bias generated by this model.","C7, C8, C9, C103",code example,,,
adept/fuyu-8b,"1. Not recommend to generate factual, or true representation of real life even or people, due to the model is not trained on such data.
2. The model can reinforce or exacerbate social biases.","C11, C106",brief mention,,,
af1tang/personaGPT,false positive,,,,,
albert-base-v2,"1. The model's output can be biased, and code samples for gender biased prediction is shown.
2. The bias can affect fine-tuned version of this model.","C12, C104",code example,,,
alefiury/wav2vec2-large-xlsr-53-gender-recognition-librispeech,false positive,,,,,
alexandrainst/scandi-nli-large,false positive,,,,,
amazon/MistralLite,"1. Before using the model, people need to perform assessment and take measures to ensure the model comply with their quality control standards, as well as rules, laws, regulations and licenses.","C33, C41, C128",,,,
amberoad/bert-multilingual-passage-reranking-msmarco,false positive,,,,,
anas-awadalla/mpt-1b-redpajama-200b,false positive,,,,,
annahaz/xlm-roberta-base-misogyny-sexism-tweets,1. The intended uses & limitations section is filled with TODO flag.,C26,,,,
apanc/russian-sensitive-topics,false positive,,,,,
asafaya/bert-base-arabic,false positive,,,,,
baichuan-inc/Baichuan-13B-Base,"1. The model is open to academic research, and can be used for commercial purpose after application.
2. Before using the model, people need to perform assessment and take measures to ensure the model comply with their quality control standards, as well as rules, laws, regulations and licenses.
3. They have tried their best to ensure compliance of the data, but risks still potentially exist. 
4. They take no responsibility for the model being misused.","C5, C19, C33",,,,
baichuan-inc/Baichuan-7B,"1. This model allows commercial use purposes.
2. It is considered irresponsible and harmful to use the model withou assessment and mitigation or risk, and the model should not be used to generate factually accurate contents.
3. This model is trained on public datasets, and could generate incorrect output or other biased, offensive or inappropriate contents although authors have made efforts to clean the data.","C5, C11, C70, C118, C119",brief mention,brief mention,,
baichuan-inc/Baichuan2-7B-Base,"1. The model is open to academic research, and can be used for commercial purpose after application.
2. Before using the model, people need to perform assessment and take measures to ensure the model comply with their quality control standards, as well as rules, laws, regulations and licenses.
3. They have tried their best to ensure compliance of the data, but risks still potentially exist. 
4. They take no responsibility for the model being misused.","C5, C19, C70",,,,
bennyguo/zero123-xl-diffusers,"1. This model can be used for research purposes of large model safe deployment, and understanding biases and limitations in generative models.
2. Creating certain types of images using this model which causing  hostile or alienating environments for people or sharing contents that violate license is considered malicious use.
3. Not recommend to generate factual, or true representation of real life even or people, due to the model is not trained on such data.
4. This model might not perform well on commnunities and cultures that use other languages than English, because the model is primarily trained on English dataset.
5. The model should be used with the safety module, which checks input of NSFW contents.","C11, C21, C22, C70, C75, C103",brief mention,,,actionable
berkeley-nest/Starling-LM-7B-alpha,false positive,,,,,
bert-base-cased,"1. The model's output can be biased, and code samples for gender biased prediction is shown.
2. The bias can affect fine-tuned version of this model.","C12, C104",code example,,,
bert-base-multilingual-uncased,"1. The model's output can be biased, and code samples for gender biased prediction is shown.
2. The bias can affect fine-tuned version of this model.","C12, C104",code example,,,
bert-large-uncased,"1. The model's output can be biased, and code samples for gender biased prediction is shown.
2. The bias can affect fine-tuned version of this model.","C12, C104",code example,,,
bigcode/starcoder,"1. The training data consists of multiple programing languages, but the natual language within the programming language is mostly English.",C112,,,,
bigscience/bloom-560m,"1. Make reference to license for usage restrictions
2. Model is not recommended to use in several high-stakes domains, having impact on individuals, or generating correct contents.
3. provide a list of usage that is considered model misuse.
4. Provide source of enegery, carbon emission and electricity usage for training this model, but is empty.","C1, C24, C25, C26, C70, C71, C72",,,brief mention,
bigscience/bloomz-3b,false positive,,,,,
bigscience/T0_3B,"1. They exclude potentially harmful contents from the dataset for finetuning the model, but the model still contains bias.
2. Provide examples of model being biased and offensive.
3. They evaluate the models bias in gender by evaluate its capability in distinguish gender bias, and reproduce gender bias","C1, C5, C104, C118, C119",evaluated result,example,example,
camembert-base,"1. Make reference to literatures for bias and fairness issues.
2. Quality for some low-resource language in the dataset are lower, and the dataset could contain sensitive and personal information.","C7, C31, C112, C131",,,,
cardiffnlp/twitter-roberta-base-emotion,false positive,,,,,
cardiffnlp/twitter-roberta-base-hate,false positive,,,,,
cardiffnlp/twitter-roberta-base-hate-latest,false positive,,,,,
cardiffnlp/twitter-roberta-base-offensive,false positive,,,,,
cardiffnlp/twitter-xlm-roberta-base-sentiment,false positive,,,,,
cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual,false positive,,,,,
cerebras/btlm-3b-8k-base,"1. This model can be used for commercial purpose.
2. This model can be used for research in ethics.
3. Safety-related testing and mitigation is required before the model being applied for downstream applications, as the output may not align well with human values.
4. The bias in the training dataset can be manifest in various forms in downstream tasks.","C21, C32, C106, C113, C128",brief mention,,,
cerebras/Cerebras-GPT-6.7B,"1. This model can be used for reearch in ethics.
2. Safety-related testing and mitigation is required before the model being applied for downstream applications, as the output may or align well with human values.
3. The bias in the training dataset can be manifest in various forms in downstream tasks, and make reference to the analysis.
4. They mitiagte the bias by standard dataset pre-processing.","C21, C32, C106, C113, C128",brief mention,,,
climatebert/distilroberta-base-climate-detector,false positive,,,,,
climatebert/distilroberta-base-climate-sentiment,false positive,,,,,
climatebert/renewable,false positive,,,,,
climatebert/transition-physical,false positive,,,,,
codellama/CodeLlama-34b-Instruct-hf,"1. This model supports commercial use purposes.
2. This model only supports use in English, and use in other languages, or any manner violates the laws, use policy and licensing agreement is considered out-of-scope.
3. Present the estimated carbon emission for the model training, and also states that the emission is offset by their substainability program.
4. Provide reference to external responsible use guide.
5. Before deployment, developers should perform safety test and tuning for their specific use, as it cannot cover all test case scenarios, and output canbe inaccurate or biased.","C1, C33, C36, C41, C128, C129",,,brief mention,
cointegrated/rubert-tiny-toxicity,false positive,,,,,
colorfulscoop/sbert-base-ja,"1. The developer of this model is not liable for any loss or damage caused by the model.
2. The model can generate biased or incorrect contents.","C1, C19, C32, C106",,,brief mention,
CompVis/stable-diffusion-safety-checker,"1. The model should not be used to do harmful thing to people.
2. Make reference to literatures for bias and fairness issues.
3. make direct link to CLIP model card: Model biases depend on class design, and disparities were found, which was exemplified with an experiment they run related to racial, gender and age, with results displayed.
4. Recommend users to be aware of the bias and risk when using.
5. Environmental impact section with the carbon emission left empty.","C7, C8, C9, C26, C70",,,,
core42/jais-13b-chat,"1. Model should not be used to generate content that harm people or go against the law.
2. Model should not handle or generate confidential information, and high-stake dicisions need to be oversight by human.
3. The model's performance in other langauge and dialects might be different.
4. The training data, although filtered, still can have bias information, and therefore the model can have biased behaviour.
5. The model can generate biased or incorrect contents, and the authors are not responsible for these consequences
6. Evaluation of the model for generation misleading/ neutrality.
7. Authors have added safety-oriented features to the model.",,,,,
csebuetnlp/mT5_multilingual_XLSum,false positive,,,,,
d4data/bias-detection-model,1. List the carbon emission for this model.,C35,,,,
damo-vilab/modelscope-damo-text-to-video-synthesis,"1. The model only supports usage in English context, not other languages.
2. Not recommend to generate factual, or true representation of real life even or people, due to the model is not trained on such data.
3. Use the model to generate content that harm people, misinformation or inappropriate content is considered model misue.","C11, C70, C71, C74",,,,
damo-vilab/text-to-video-ms-1.7b,"1. The model only supports usage in English context, not other languages.
2. Not recommend to generate factual, or true representation of real life even or people, due to the model is not trained on such data.
3. Use the model to generate content that harm people, misinformation or inappropriate content is considered model misue.","C11, C70, C71,  C74",,,,
daryl149/llama-2-7b-chat-hf,"related, but we have a look later",,,,,
databricks/dolly-v2-7b,"
1. The model has shortcoming in factual errors, hallucinations.
2. The training data contains objectionable contents, causing the model to sometimes generate them (general comments)
3. The training data reflects the interests/ perceptions of annotators, which may not be representative of global population.","C1, C39, C106, C121",brief mention,,brief mention,
Davlan/bert-base-multilingual-cased-ner-hrl,1. The model may not generate to other domains as it is trained on only news articles in a specific time frame.,C114,,,,
Davlan/xlm-roberta-large-ner-hrl,1. The model may not generate to other domains as it is trained on only news articles in a specific time frame.,C114,,,,
declare-lab/flan-alpaca-large,false positive,,,,,
DeepFloyd/IF-II-M-v1.0,"1. Not recommend to generate factual, or true representation of real life even or people, due to the model is not trained on such data.
2. Use this model to generate content that harm people or violate the license is considered as misuse.
3. The training data is mainly in English, so the model does not work as well in other langauges, and could overrepresent white and western culstures.
4. The training dataset contains inappropriate (violent and sexual) contents.
5. Make reference to blogs about author's mitigation method.","C11, C40, C41, C70, C71, C74, C75, C103, C112, C122, C123",brief mention,,,
defog/sqlcoder2,"1. License on the model weights alllows commercial use, but requires the modified weights to be open source under the same license terms.",C41,,,,
deutsche-telekom/gbert-large-paraphrase-cosine,1. Users cannot use this model unless comply with the license.,C41,,,,
distilbert-base-cased,"1. The model's output can be biased, and code samples for gender and ethnical biased prediction is shown.
2. The bias can affect fine-tuned version of this model.
3. refer to further bias information in other documents.","C12, C42,C104, C105, C131",code example,,,
distilbert-base-multilingual-cased,"1. Models's output can be harmful and contain bias, which the users of this model should be aware of, and make reference to related literatures
2. The model's performance varies among different languages, and the results for each language is displayed.
3. Environmental impact report with hardware, computing hour, computing provider, and carbon emission. (empty entry)","C7, C8, C9, C26, C106, C107",brief mention,,,
distilbert-base-uncased,"1. The model's output can be biased, and other bias can be found in its teacher model (provide an reference).
2. The example bias is exmplified in code samples, with racial bias.
3. Bias will affect downstream fine-tuned version of the model.","C12, C42, C104, C105, C131",code example,,,
distilbert-base-uncased-finetuned-sst-2-english,"1. The model should not be used to do harmful thing to people.
2. Not recommend to generate factual, or true representation of real life even or people, due to the model is not trained on such data.
3. The model contain biased towards underrepresented population, which was exmplified in an example, with probability listed, and additional notebook shown.
4. Recommend users to watch and evaluate the bias for their use cases, and provide relevant resources (dataset)","C11, C45, C103, C128",example,,,actionable suggestion
distilgpt2,"1. The model behaviour reflect data it was trained on, and contain potential bias, which is exeplify using code on racial bias. 
2. The model does not support use cases of true generation text, because the model incapability of distinguish fact from fiction.
3. Does not recommend to be deployed for human-interation unless conducting study on relevant bias.
4. Environmental impact report with hardware, computing hour, computing provider, and carbon emission.","C7, C11, C35, C57, C105, C128",code example,,,
dreamlike-art/dreamlike-diffusion-1.0,"1. The model can be used for commercial purposes, with certain terms attached.
2. If re-distribute the model weights, requiring to include the same use restrictions in the license.",C41,,,,
dreamlike-art/dreamlike-photoreal-2.0,"1. The model can be used for commercial purposes, with certain terms attached.
2. If re-distribute the model weights, requiring to include the same use restrictions in the license.",C41,,,,
DTAI-KULeuven/robbert-v2-dutch-sentiment,"1. For the dataset of book reviews, most reviews are from woman, which could result in different performance in gender.
2. Make a reference to external material on performance difference in reviews written by man and woman.","C7, C104, C115",brief mention,,,
ehartford/dolphin-2.1-mistral-7b,"1. The author filtered the dataset for bias, which makes the model more compliant, but it is still uncensored model.
2. Recommend the user to implement own alignment layer before providing this model as a service.
3. Users are responsible for any use and content.
4. The model can be used for both commercial and non-commercial use cases.","C5, C19, C22, C40, C46",,,,actionable
ehartford/dolphin-2.2.1-mistral-7b,"1. The author filtered the dataset for bias, which makes the model more compliant, but it is still uncensored model.
2. Recommend the user to implement own alignment layer before providing this model as a service.
3. Users are responsible for any use and content.
4. The model can be used for both commercial and non-commercial use cases.","C5, C19, C22, C40, C46",,,,actionable
ehartford/dolphin-2.5-mixtral-8x7b,"1. The author filtered the dataset for bias, which makes the model more compliant, but it is still uncensored model.
2. Recommend the user to implement own alignment layer before providing this model as a service.
3. Users are responsible for any use and content.
4. The model can be used for both commercial and non-commercial use cases.","C5, C19, C22, C40, C46",,,,actionable
ehartford/Samantha-1.11-70b,false positive,,,,,
EleutherAI/gpt-j-6b,"1. Explain how GPT makes the statistically most likely prediction, not the most accurate text, therefore, don't depend on the model to generate factual content.
2. Recommend to evaluate and mitigate the risks associated with particular use cases.
3. Offensive content might be generated by the model, and recommend to having huamn curate or filter before releasing.","C1, C22, C78, C122, C124, C128",,,,actionable
EleutherAI/gpt-neo-2.7B,"1. Explain how GPT makes the statistically most likely prediction, not the most accurate text, therefore, don't depend on the model to generate factual content.
2. The trained dataset contains inappropriate content
3. Offensive content might be generated by the model, and recommend to having huamn curate or filter before releasing.","C1, C22, C122, C124",,,,actionable
EleutherAI/gpt-neox-20b,"1. This model is trained only in English text, so not suitable for translation tasks, or used in other langauges.
2. Explain how GPT makes the statistically most likely prediction, not the most accurate text, therefore, don't depend on the model to generate factual content.
3. The trained dataset contains inappropriate content
4. Offensive content might be generated by the model, and recommend to having huamn curate or filter before releasing.","C1, C22, C122, C124",,,,actionable
EleutherAI/polyglot-ko-1.3b,"1. The data collection for the training data is abided South Korean laws, and not released for public use.
2. Offensive content might be generated by the model, and recommend to having huamn curate or filter before releasing.
3. Explain how GPT makes the statistically most likely prediction, not the most accurate text, therefore, don't depend on the model to generate factual content.","C1, C22, C48",,,,actionable
EleutherAI/pythia-410m-deduped,"1. This model can be used for promoting interpretability research.
2. The model may generate harmful content, and thus cannot be directly deployed without moderation, or sued for human-facing interations. Recommend users to evaluate the risks for the cases.
3. This model is trained only in English text, so not suitable for translation tasks, or used in other langauges.
4. Explain how GPT makes the statistically most likely prediction, not the most accurate text, therefore, don't depend on the model to generate factual content.
5. The trained dataset contains inappropriate content, and provide reference for the biased content in the dataset.
6. Offensive content might be generated by the model, and recommend to having huamn curate or filter before releasing.","C1, C7, C21, C22, C50, C119, C122, C124, C128",,brief mention,,actionable
EleutherAI/pythia-6.9b-deduped,"1. The model may generate harmful content, and thus cannot be directly deployed without moderation, or sued for human-facing interations. Recommend users to evaluate the risks for the cases.
2. This model is trained only in English text, so not suitable for translation tasks, or used in other langauges.
3. Explain how GPT makes the statistically most likely prediction, not the most accurate text, therefore, don't depend on the model to generate factual content.
4. The trained dataset contains inappropriate content, and provide reference for the biased content in the dataset.
5. Offensive content might be generated by the model, and recommend to having huamn curate or filter before releasing.","C1, C7, C22, C50, C119, C122, C124, C128",,,,actionable
euclaise/Ferret_7B,false positive,,,,,
facebook/bart-large-cnn,false positive,,,,,
facebook/dpr-question_encoder-single-nq-base,"1. Not recommend to generate factual, or true representation of real life even or people, due to the model is not trained on such data.
2. Use model to harm people is considered as model misuse.
3. The model output can contain harmful stereotypes, and further provide reference to bias and fairness discussions. Also make reference to ethical related literatures.
4. Environmental impact for carbon emission is listed but left blank.","C7, C11, C26, C70, C106, C107",brief mention,,,
facebook/flava-full,"1. The model intends to provide reproducibillity, enhance researchers' understanding in model robustness and biases.
2. Does not recommend any deployment use cases
3. Not recommended for non-deployed use cases such as image search, unless perform fixed class taxonomy in-domain test, due to varied performance of the model, and its potential harm.
4. Uses case for surveillance and facial recognition are out of scope, due to lack of testing norms and fair use check.
5. Should only be used in English, as it is not purposefully trained or evalutaed in other language.
6. The training dataset are from internet, meaning that it has bias towards people most connected to the internet.","C21, C50, C72, C78, C116, C130",,,,
facebook/mms-1b-all,false positive,,,,,
facebook/musicgen-stereo-medium,"1. THe model should not be used for downstream task before risk evaluation and mitigations.
2. The model should not be used for harm people, or generate biased or offensive contents.
3. The model potentially lacks diversity in music styles, and might not works well in other languages compared to English description.
4. The model might generate biased or offensive content, which motivate the authors to provide reproducible research.","C21, C70, C106",brief mention,,,
facebook/nllb-200-3.3B,"1. Make reference for the carbon emission for this model.
2. The model might not capture some variations in the languages, and users should make assessment.
3. The developers of this model make efforts to prioritize human users and minimise their risks.
4. The model can be used to improve education and information access in some communities, as many languages chosen are low-resource languages, especially in African countries. However those people might also prone to fraught.
5. The data is from various public sources. Although cleaned, there can still exist some personal information.
6. Translation quality (mistranslation) of the model still remains, which could adverse impact people replying on it.","C4, C31, C35, C50, C71, C127",,,,
facebook/opt-1.3b,"1. The model can be used for research in reproducibility, bias, risk harm toxicity etc.
2. The training data contains unfiltered biased contents, while the model output have diversity and hallucination issues.
3. Provide code demo for gender bias output for this model.
4. The bias can affect fine-tuned version of this model.","C1, C12, C21, C104, C124",code example,,brief mention,
facebook/opt-iml-max-1.3b,"1. The model has potential risk in factual corretness, generation of harmful and stereotypical output, and the model should be use responsibly.","C1, C54, C106, C119",brief mention,,brief mention,
facebook/roberta-hate-speech-dynabench-r4-target,false positive,,,,,
facebook/sam-vit-huge,false positive,,,,,
facebook/wmt19-de-en,false positive,,,,,
Falconsai/medical_summarization,1. Users should use the model responsibly and following the guidelines for real-world applications.,C54,,,,
Falconsai/nsfw_image_detection,1. Users should use the model responsibly and following the guidelines for real-world applications.,C54,,,,
Falconsai/text_summarization,1. Users should use the model responsibly and following the guidelines for real-world applications.,C54,,,,
FFusion/FFXL400,"1. The model is not intended for commercial usage, except a few options.
2. The development team is not responsible for the content produced by this model.",C19,,,,
flair/ner-english-ontonotes-large,false positive,,,,,
FredZhang7/distilgpt2-stable-diffusion-v2,false positive,,,,,
garage-bAInd/Platypus2-7B,"1. Before deployment, developers should perform safety test and tuning for their specific use, as it cannot cover all test case scenarios, and output canbe inaccurate or biased. Also, the test is only conducted in English.
2. Make reference to external responsible use guide.","C1, C36, C106, C120, C128, C129",brief mention,,brief mention,
google/flan-t5-base,"1. Not recommended to directly used in application, due to potential harmful generation of model, unless with assessment of safty and fairness specific to application.
2. Model potentially will generate inappropriate content due to large corpus of data it is fine-tuned on is not filtered.
3. This model is not tested in real world applications and should not be applied for harmful use cases.
4. Make reference to other model card.","C57, C70, C120, C121, C128",,brief mention,,
google/flan-ul2,false positive,,,,,
google/owlvit-base-patch32,"1. The model can be used by researchers to understand robustness, and biases factors of machine learning models.
2. The dataset is collected from internet, meaning it is representing people most connected to the internet.","C21, C116",,,,
google/tapas-base-finetuned-wtq,false positive,,,,,
gpt2,"1. The training data contains biased unfiltered data, which reflect in the model. Therefore, not recommend to interact with human unless carry out assessment relevant to the use cases.
2. Do not use model to distinguish fact and fiction, as the model does not support factual generation.
3. The bias in different size checkpoints of this model are not statistically significant.
4. The bias can affect fine-tuned version of this model.
5. Provide code example for gender and race bias output of the model.","C11, C12, C57, C91, C105, C125, C128",code example,,,
gpt2-large,"1. The training data contains biased unfiltered data, which reflect in the model. Therefore, not recommend to interact with human unless carry out assessment relevant to the use cases.
2. Do not use model to distinguish fact and fiction, as the model does not support factual generation.
3. The bias in different size checkpoints of this model are not statistically significant.
4. Provide code example on gender bias of this model.
5. Provide reference to research on bias and fairness.
6. The bias can affect fine-tuned version of this model.","C7, C11, C12, C57, C91, C104, C125, C128",code example,,,
gpt2-xl,"1. The training data contains biased unfiltered data, which reflect in the model. Therefore, not recommend to interact with human unless carry out assessment relevant to the use cases.
2. Do not use model to distinguish fact and fiction, as the model does not support factual generation.
3. The bias in different size checkpoints of this model are not statistically significant.
4. Provide code example on gender bias of this model.
5. Provide reference to research on bias and fairness.
6. The bias can affect fine-tuned version of this model.
7. The model can be further finetuned to propagate extreme contents, and suggest ML-based detection.
8. Environmental impact report with hardware, computing hour, computing provider, and carbon emission. (empty entry)","C7, C11, C12, C22, C26, C57, C77, C91, C104, C125, C128",code example,,,actionable
Gryphe/MythoMax-L2-13b,false positive,,,,,
haining/scientific_abstract_simplification,"1. The generated text may not accurately reflect the contents, therefore requiring huamn experts when making important decisions.","C1, C22",,,brief mention,actionable
Hate-speech-CNERG/bert-base-uncased-hatexplain,false positive,,,,,
Helsinki-NLP/opus-mt-en-de,1. Make reference to literatures for bias and fairness issues.,C7,,,,
Helsinki-NLP/opus-mt-tc-base-en-sh,"1. The training data may contain harmful contents, and the model could generate stereotypes and biased contents.
2. Make reference to literatures for bias and fairness issues.","C7, C106, C124",brief mention,,,
hkunlp/instructor-xl,false positive,,,,,
HooshvareLab/bert-fa-base-uncased-sentiment-deepsentipers-binary,false positive,,,,,
hotshotco/Hotshot-XL,1. The model generation could reinforce social biases.,C106,brief mention,,,
HuggingFaceH4/zephyr-7b-beta,"1. The model is not aligned with human preference for safety, and therefore can produce problematic outputs.
2. For the base model that this model developed on, it is not sure what data it is trained on, and provide reference to the original model card.","C57, C59",,,,
HuggingFaceM4/idefics-9b-instruct,"1. Hardware, hours and carbon emission for training the models.
2. Make reference to literatures for bias and fiarness issues.
3. Provide examples for the model to generate incorrect or offensive contents, and provide example for the model's behaviour when coping with some ethical considerations.
4. Provide evaluation of the gender, ethnicity bias for this model.
5. The training dataset contains objectionable (sexual and violent) contents, which could lead the model to generating such contents
6. Using the model to generate medical diagonisis should be out-of-scope, unless after assessment and adaption.
7. This model cannot be used for high-stake settings, which includes critical decisions and evaluating individuals.
8. Generating contents that harm people is considered model-misuse.","C1, C7, C11, C25, C35, C46, C70, C71, C72, C104, C105, C111, C122, C123",evaluated result,,example,
iahlt/span-marker-xlm-roberta-base-ar,false positive,,,,,
IMSyPP/hate_speech_en,false positive,,,,,
Intel/distilbert-base-uncased-finetuned-sst-2-english-int8-static,"1. Have an empty entry for environment impact of the model.
2. The training data are movie reviews from authors on the internet, can be biased towards the reviewers' opinion and lebeler of the data.
3. This model should not inform decisions to human life.
4. Authors did not perform any risk mitigation strategies during model development.
5. Make reference to literatures for bias and fairness issues.
6. The model can generate harmful content, and recommend users to be made aware of the risks biases and limitations of the model.","C7, C19, C25, C26, C62",,,,
Intel/dpt-hybrid-midas,"1. The model shoudl not be used to harm people, and not intended to make decisions central to human life.
2.  Authors did not perform any risk mitigation strategies during model development.","C25, C62, C70",,,,
Intel/neural-chat-7b-v3-1,"1. The model cannot be relied to generate factual information.
2. Because of the finetuning dataset as well as the pretrained model, this model could generate harmful and biased contents.
3. The authors are not responsible for how this model is used.","C1, C19, C106, C118, C119, C121",brief mention,,brief mention,
Isotonic/distilbert_finetuned_ai4privacy,false positive,,,,,
jbochi/madlad400-3b-mt,"1. The training set contain sensitive or harmful content, it might have an impact on the model output.
2. Make reference to literatures for risks issues.
3. model sensitive use section and environmental impact sections are left empty.
4. Model is only evaluated on machine translation tasks, and users should consider for their usecase.","C7, C26, C31, C119, C121, C124",,brief mention,,
JiaqiLee/imdb-finetuned-bert-base-uncased,false positive,,,,,
jonfd/electra-small-nordic,1. The dataset is split equally between different language sources.,C127,,,,
JujoHotaru/lora,false positive,,,,,
kakaobrain/align-base,"1. List the training dataset, filtering applied on them, and whether some sensitive data exist in the dataset.
2. The model can be used by researchers to understand robustness, and biases factors of machine learning models.","C5, C21",,,,
klue/bert-base,"1. Not recommend to generate factual, or true representation of real life even or people, due to the model is not trained on such data.
2. Use the model to generate content that harm people, misinformation or inappropriate content is considered model misue.
3. Make reference to literatures for risks issues.
4. Training dataset contains bias, and PII exist in the data, although efforts made to pseudonymise them.
5. Environmental impact report with hardware, computing hour, computing provider, and carbon emission. (empty entry)","C7, C11, C26, C31, C40, C70, C125",,,,
KoalaAI/Text-Moderation,"1. The model is trained on English, and may not perform as well in other languages.
2. The model may amplify biases in the training data, and give an example of such case. This can result in unfairness in certain groups.
3. Users should take actions to mitigate the harm by considering its impact.
4. Users should respect the privacy of the data subject, and comply to laws.
5. The model developer is not responsible for any damage this model might have.","C33, C106, C130",brief mention,,,
KoboldAI/GPT-J-6B-Janeway,"1. Explain how GPT makes the statistically most likely prediction, not the most accurate text, therefore, don't depend on the model to generate factual content.
2. The trained dataset contains inappropriate content
3. Offensive content might be generated by the model, and recommend to having human curate or filter before releasing.
4. Make reference to literatures on the bias in the dataset.","C1, C7, C22, C118, C119, C122, C124",,brief mention,explain,actionable
KoboldAI/OPT-13B-Nerys-v2,1. Mention potential bias factors for readers to be aware.,"C104, C105, C107, C108",brief mention,,,
KoboldAI/OPT-2.7B-Erebus,"1. This model generates X-rated contents, and not suitable for minors to use.
2. Mention potential bias factors for readers to be aware.","C64, C104, C105, C107, C108, C118",brief mention,brief mention,,
laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg,"1. Does not recommend any deployment use cases
2. Not recommended for non-deployed use cases such as image search, unless perform fixed class taxonomy in-domain test, due to varied performance of the model, and its potential harm.
3. Uses case for surveillance and facial recognition are out of scope, due to lack of testing norms and fair use check.
4. Should only be used in English, as it is not purposefully trained or evalutaed in other language.
5. The dataset is uncurated, which contains NSFW contents. Even for the filtered dataset, NSFW contents cannot be entirely exlucded.
5. Does not recommend to produce ready-to-go industrial products, as the research is still in progress.","C21, C50, C72, C78, C122",,,,
lambdalabs/sd-image-variations-diffusers,"1. Model can be used for understanding biases in generative models, and safe deployment of the model.
2. Use models to harm people,  propagate stereotypes, generate inappropriate contents, violate license,  is considered as model misuse.
3. This model might not perform well on commnunities and cultures that use other languages than English, because the model is primarily trained on English dataset.
4. The model should be used with the safety module, which checks input of NSFW contents.","C21, C22, C70, C103, C112",brief mention,,,actionable
LeoLM/leo-hessianai-7b-chat,"1. Before deployment, developers should perform safety test and tuning for their specific use, as it cannot cover all test case scenarios, and output canbe inaccurate, biased or objectional.
2. Make reference to responsible use guide document.","C1, C36, C106, C120, C128, C129",brief mention,brief mention,brief mention,
lewdryuna/A-Rainier,"1. Publish NSFW content in public places is not the intended use case for the model, and it is the users responsibility to create SFW and NSFW images.","C19, C74",,,,
Linaqruf/animagine-xl,false positive,,,,,
Linaqruf/animagine-xl-2.0,"1. The model is finetuned on data with similar aesthetic, an result in bias towards a specific art style.
2. The model is not good at generating NSFW content, with intetion to promote safe and appropriate content generation.","C4, C109, C117",brief mention,,,
lllyasviel/control_v11p_sd15_lineart,1. The license for this model is based on responsible AI licensing.,C41,,,,
lllyasviel/control_v11p_sd15_softedge,1. The license for this model is based on responsible AI licensing.,C41,,,,
llm-jp/llm-jp-1.3b-v1.0,"1. The model is still in early stages of research, and not tuned to align with human considerations for safety.",C59,,,,
llmrails/ember-v1,false positive,,,,,
madebyollin/sdxl-vae-fp16-fix,false positive,,,,,
manueldeprada/FactCC,false positive,,,,,
martin-ha/toxic-comment-model,"1. The model's output classification can be bias towards people of Muslim, which provide evaluated metrics for different groups of people, and provide a example sentence that can trigger this bias information.","C46, C103, C104, C105, C110",evaluated result,,,
meta-llama/Llama-2-70b-hf,"1. Model should not be used to voilate laws, used in languages other than English, or breaking use policy and license agreement.
2. The mode's output can be inaccurate or biased due to test not able to cover all cases, and only conducted in English, thus requiring safty testing and tuning before deloyment.
3. Make reference to responsible use guide.","C33, C36, C41, C128, C129",,,,
michellejieli/NSFW_text_classifier,false positive,,,,,
microsoft/biogpt,false positive,,,,,
microsoft/Orca-2-13b,"1. This model is evaluated in safety perspective, and details are made reference to the paper.
2. Training data is synthetic, and content filters are applied to the synthetic data, and details are made reference to the paper. The training data could carry bias, and impact models output.
3. The model has limited real-world understanding, resulting in potential inaccurate or nonsensical output. It suffers from hallucination, and the impact on small models is not sure.
4. Limited transparency on model behaviour and interpretation.
5. The model can be used for generating harmful content. And the authors advocate for better regulation.","C1, C5, C40, C65, C66, C67, C70, C71, C125",,,brief mention,
microsoft/phi-1_5,"1. The model can be used for research  purpose to reduce bias and toxicity of the model.
2. They exclude trainng data crawled online to prevent harmful online content. However, the model can still generate harmful and biased content, requiring the users to exercise with caution.
3. The model is designed to understand standard English, other languages or dialects is not going to perform well.","C1, C9, C21, C106, C120, C127",brief mention,brief mention,,
microsoft/phi-2,"1. The model can be used for research  purpose to reduce bias and toxicity of the model.
2. They exclude trainng data crawled online to prevent harmful online content. However, the model can still generate harmful and biased content, requiring the users to exercise with caution.
3. The model is designed to understand standard English, other languages or dialects is not going to perform well.
4. The model cannot be directly adopted for production without testing.","C1, C9, C21, C91, C98, C106, C120, C127",brief mention,brief mention,,
microsoft/speecht5_tts,"1. Bias, risks and limitations section, recommendations section, out-of-scope section and environmental impact section all left empty.",C26,,,,
microsoft/swinv2-large-patch4-window12to16-192to256-22kto1k-ft,false positive,,,,,
microsoft/unispeech-sat-base,false positive,,,,,
microsoft/unixcoder-base,"1. out-of-scope use section, environmental impact section left empty.
2. Make reference to literatures for ethical considerations. This model can also generate harmful content, and users should be made aware of them.","C7, C106, C107",brief mention,,,
microsoft/wavlm-base-plus,false positive,,,,,
Minej/bert-base-personality,"1. Use the model for critical decision such as employment, education and legal matters are out of scope.
2. The model is limited by its context, factors other than text can impact personality traits. 
3. The model learns pattern from the training dataset, and may vary when applied to other demographic or cultural backgrounds.
4. Users should be cautious on sharing their personal data.
5. To mitigate the risk of using the model, users should understand 2, 3 and 4.","C25, C32, C54",,,,
minimaxir/sdxl-wrong-lora,false positive,,,,,
mistralai/Mistral-7B-Instruct-v0.1,1. They look forward to make the model respect guardrails in the future.,C69,,,,
Mitsua/mitsua-diffusion-cc0,"1. The model should not be used to generate harmful content.
2. Authors claim no rights to the model output, and users are accountable for the model output.
3. Users need to use the same license if they intend to re-distribute the model weights and use for commercial purpose or as a service.","C19, C41",,,,
ml6team/keyphrase-extraction-distilbert-inspec,false positive,,,,,
MonoHime/rubert-base-cased-sentiment-new,"1. The model should not be used to do harmful thing to people.
2. Make reference to literatures for bias and fairness issues. The prediction may have harmful stereotypes to certain groups.
4. Recommend users to be aware of the bias and risk when using.
5. Further recommendation is left pending.
6. Environmental impact section is left blank with template.","C7, C8, C9, C26, C70, C106, C107",,,,
MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli,"1. Make reference to the paper for the model and dataset for ethical issues, stating that the model output will reproduce statistical pattern in training data.",C40,,,,
MoritzLaurer/deberta-v3-large-zeroshot-v1,1. Make reference to the paper for the model and dataset for ethical issues.,C40,,,,
MoritzLaurer/DeBERTa-v3-xsmall-mnli-fever-anli-ling-binary,1. Make reference to the paper for the model and dataset for ethical issues.,C40,,,,
MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7,"1. Make reference to the paper for the model and dataset for ethical issues.
2. The dataset compsed of data created using machine translation, which reduce the data quality.",C40,,,,
mosaicml/mpt-30b,"1. The model may generate harmful content, and thus cannot be directly deployed without moderation, or used for human-facing interations.
2. The model cannot be used for producing factually accurate information, and because training on public dataset (which was cleaned), it still could generate inappropriate contents.
3. The license does not constitute legal advice, and authors are not responsible for any use of the model.","C5, C11, C19, C78, C118, C119",,brief mention,,
mosaicml/mpt-30b-instruct,"1. The model may generate harmful content, and thus cannot be directly deployed without moderation, or used for human-facing interations.
2. The model cannot be used for producing factually accurate information, and because training on public dataset (which was cleaned), it still could generate inappropriate contents.
3. The license does not constitute legal advice, and authors are not responsible for any use of the model.","C5, C11, C19, C78, C118, C119",,brief mention,,
mosaicml/mpt-7b,"1. The model may generate harmful content, and thus cannot be directly deployed without moderation, or used for human-facing interations.
2. The model cannot be used for producing factually accurate information, and because training on public dataset (which was cleaned), it still could generate inappropriate contents.
3. The license does not constitute legal advice, and authors are not responsible for any use of the model.","C5, C11, C19, C78, C118, C119",,brief mention,,
mosaicml/mpt-7b-chat,"1. The model may generate harmful content, and thus cannot be directly deployed without moderation, or used for human-facing interations.
2. The model cannot be used for producing factually accurate information, and because training on public dataset (which was cleaned), it still could generate inappropriate contents.
3. The license does not constitute legal advice, and authors are not responsible for any use of the model.","C5, C11, C19, C78, C118, C119",,brief mention,,
mrm8488/t5-base-finetuned-span-sentiment-extraction,1. The dataset contains inappropriate content.,"C122, C124",,,,
mshenoda/roberta-spam,false positive,,,,,
mtg-upf/discogs-maest-30s-pw-73e-ts,"1. This model is under non-comercial application, and proprietary license.
2. The training dataset is an overrepresentation of western music (diversity not good enough)
3. List the carbon emission for training this model.",C117,,,,
nateraw/bert-base-uncased-emotion,false positive,,,,,
nbroad/ESG-BERT,"1. The model should not be used to do harmful thing to people.
2. Make reference to literatures for bias and fairness issues. The prediction may have harmful stereotypes to certain groups.
4. Recommend users to be aware of the bias and risk when using.
5. Environmental impact section is left blank with template.","C7, C8, C9, C26, C70, C106, C107",brief mention,,,
NeuML/t5-small-txtsql,false positive,,,,,
nitrosocke/Ghibli-Diffusion,"1. The model should not be used to generate harmful content.
2. Authors claim no rights to the model output, and users are accountable for the model output.
3. Users need to use the same license if they intend to re-distribute the model weights and use for commercial purpose or as a service.","C19, C41",,,,
nlpaueb/legal-bert-base-uncased,false positive,,,,,
NousResearch/Nous-Hermes-llama-2-7b,"1. The model is trained on synthetic dataset from GPT-4 outputs.
2. In the future, the plan to give more high quality training data, and filter low quality data.",C69,,,,
ntu-spml/distilhubert,false positive,,,,,
nvidia/speakerverification_en_titanet_large,false positive,,,,,
olm/olm-roberta-base-dec-2022,1. The training dataset for this model is cleaned.,C5,,,,
openai-gpt,"1. Not recommend to generate factual, or true representation of real life even or people, due to the model is not trained on such data.
2. Make reference to literature on the bias and fairness issues.
3. The model's output can be biased, and code samples for gender biased prediction is shown.
4. The bias can affect fine-tuned version of this model.
5. The training data does not contain complete information about the world, and therefore, and have bias.
6. This model is more robust, but still exhibit conterintuitive behaviour.The evaluation of the robustness is provided on a dataset.
7. The environmental impact section, for carbon emission, it is left empty.","C7, C11, C12, C26, C40, C79, C104, C116",code example,,,
openai/clip-vit-base-patch16,"1. Does not recommend any deployment use cases
2. Not recommended for non-deployed use cases such as image search, unless perform fixed class taxonomy in-domain test, due to varied performance of the model, and its potential harm.
3. Uses case for surveillance and facial recognition are out of scope, due to lack of testing norms and fair use check.
4. Model biases depend on class design, and disparities were found, which was exemplified with an experiment they run related to racial, gender and age, with results displayed.
5. Aspiration of this model to be used for research in biases and robustness of the model.","C50, C78, C128",,,,
openai/shap-e,1. Make reference to the original model card for limitations and bias of this model.,C57,,,,
openai/whisper-base,"1. The model can be used for research purpose to study model robustness and biases.
2. Use this model to transcribe recordings of individuals without consent or for any subjective classification like infer human attriobutes is malicious use. It is not recommend to use this model for high-risk domains.
3. The model should be assessed forc robustness before deploying them in a particular context.
4. THe model is trained in weakly supervised mannaer and the data is noisy, thus leading to hallucination behaviour of the model.
5. The model performs different for different languages, accents and dialects, particular for lower-resource languages.
6. The model has potential harmful use scenarios, such as survelliance purposes, which raise safety concerns.","C1, C21, C25, C40, C72, C103, C112, C128",brief mention,,explain,
OpenBuddy/openbuddy-zephyr-7b-v14.1,"1. The model potentially produce harmful or erroneous content, and should not be used in high-stakes situations, such as medical field, controlling software and hardware systems.
2. The use of this model is at the user's stake, and should not influence the authors of this model.","C1, C19, C25, C119",,brief mention,brief mention,
openchat/openchat_3.5,"1. The model can generate hallucination information, and users should verify any critical information obtained from the model.
2. The model could generate harmful and biased content, making it crucial to apply AI safety measure.","C1, C9, C22, C106, C119, C130",brief mention,brief mention,brief mention,actionable
openlm-research/open_llama_7b_v2,1. This model as well as the model weights are with permissive license open source model.,C41,,,,
optimum/distilbert-base-uncased-finetuned-sst-2-english,"1. The model can produce biased predictions for underrepresented population. An example is made in movie making for different countries.
2. It is recommended that users to evaluate the risk and biases on three datasets.","C45, C103, C128",example,,,actionable suggestion
optimum/gpt2,false positive,,,,,
PAIXAI/Astrid-1B,"1. The training data for this model contains biased or harmful contents, but the developers do not support this.
2. The content generated can be incorrect, and it is up to the users to evaluate the generated content.
3. The authors are not responsible for any damage caused by using this model, and users are supposed to use the model responsibly and ethically, not intentionally creating harmful content.
4. Encourage the users to report ethical issues to the maintainers of this model, and they will be used for further improvement of the model.","C1, C19, C22, C83, C121, C123, C125",,,brief mention,actionable
pankajmathur/orca_mini_3b,"1. The model cannot be used for producing factually accurate information, and because training on public dataset (which was cleaned), it still could generate inappropriate contents.
2. The license does not constitute legal advice, and authors are not responsible for any use of the model.","C5, C11, C19, C41",,,,
patrickjohncyh/fashion-clip,"1. The model inherit limitations and bias in CLIP model, and the training data have explicit notions of gender.","C42, C125",,,,
pdelobelle/robbert-v2-dutch-base,"1. Make reference to literature about discussed bias for the model
2. Provide evaluated gender bias for this model.","C40, C104",evaluated result,,,
pedramyazdipoor/persian_xlm_roberta_large,1. The training dataset is filtered.,C5,,,,
petals-team/StableBeluga2,"1. The mode's output can be inaccurate or biased due to test not able to cover all cases, and only conducted in English, thus requiring safty testing and tuning before deloyment.","C1, C106, C128, C129",brief mention,,brief mention,
Phind/Phind-CodeLlama-34B-v2,"1. The model underwent limited testing, and need to go through safety test before deployment.","C1, C106, C128, C129",,,,
pierreguillou/ner-bert-large-cased-pt-lenerbr,false positive,,,,,
PlanTL-GOB-ES/roberta-base-bne,"1. The model is not wel evaluated in bias perspective, and the authors intend to do it as future work.
2. Provide code exmpale for biased output for this model.
3. It is the users' responsibility to comply with corresponding regulations when using this model.
4. The authors of this model is not liable for any harmful contents raise by any use.","C19, C33, C69, C105",code example,,,
PrimeQA/tydiqa-boolean-answer-classifier,1. The bias in the pre-existing model may present in this fine-tuned model.,C42,,,,
princeton-nlp/Sheared-LLaMA-1.3B,1. This model should also comply with the llama2 model license.,C24,,,,
princeton-nlp/sup-simcse-roberta-large,"1. out-of-scope use section, environmental impact section left empty. 
2. Make reference to literatures for ethical considerations. This model can also generate harmful content, and users should be made aware of them.",C26,,,,
princeton-nlp/unsup-simcse-bert-base-uncased,"1. out-of-scope use section, environmental impact section left empty. 
2. Make reference to literatures for ethical considerations. This model can also generate harmful content, and users should be made aware of them.",C26,,,,
prithivida/parrot_paraphraser_on_T5,false positive,,,,,
pszemraj/led-large-book-summary,false positive,,,,,
pszemraj/long-t5-tglobal-base-sci-simplify-elife,false positive,,,,,
pyannote/embedding,false positive,,,,,
PygmalionAI/pygmalion-2-13b,"1. The model is freely available for both commercial and non-commercial use.
2. The traing data contain inappropriate contents, and the model could generate harmful contents, or factually wrong contents.","C1, C118, C119, C122, C124",,brief mention,brief mention,
pysentimiento/robertuito-emotion-analysis,false positive,,,,,
pysentimiento/robertuito-hate-speech,false positive,,,,,
pysentimiento/robertuito-sentiment-analysis,false positive,,,,,
Rakib/roberta-base-on-cuad,1. Environmental impact of carbon emission left blank.,C26,,,,
Riiid/sheep-duck-llama-2,"1. Before deployment, developers should perform safety test and tuning for their specific use, as it cannot cover all test case scenarios, and output can be inaccurate or biased.
2. Provide reference to external responsible use guide.
3. This model should follow llama license, and provide no gurantees","C24, C36, C128, C129",,,,
roberta-base-openai-detector,"1. Use this model to flag academic misconduct is out-of-scope, as this model is not accurate enough, and need to be used accompanied with non-automated approach
2. This model should not be used to create harmful content to people, and one misuse is use this model to evade detection.
3. Users should be made aware of model's risk.
4. Make reference to literatures discussing the potential bias issues of this model.
5. Environmental impact section, with the carbon emission left empty.","C26, C40, C70, C84",,,,
roberta-large,"1. Training data for this model contains un-neutral contents from internet.
2. The model's output can be biased, and code samples for gender biased prediction is shown.
3. The bias can affect fine-tuned version of this model.","C12, C104, C125",code eample,,,
roberta-large-mnli,"1. Not recommend to generate factual, or true representation of real life even or people, due to the model is not trained on such data.
2. The environmental impact section, with carbon emission left empty.
3. Make reference to bias and fairness issue literatures, and provide code example for the bias output of this model.
4. Recommend to make users be aware of the bias and risk.","C7, C8, C9, C11, C26, C57",,,,
runwayml/stable-diffusion-inpainting,"1. Not recommend to generate factual, or true representation of real life even or people, due to the model not trained with this behaviour.
2. Various malicious use of the model are listed to be model misuse.
3. Model's behaviour on other languages migth be insufficient and only represent western culture (which can reinforce social biases), as it is trained on a dataset consists of primarily English descriptions.
4. Environmental impact report with hardware, computing hour, computing provider, and carbon emission.","C11, C35, C70, C74, C75, C103, C112",brief mention,,,
Salesforce/blip2-flan-t5-xxl,"1. This model uses one of the language model as its language model component, and therefore inherits the same risk and limitation of potentially generating harmful content, and cannot be deployed into applications without safety and fairness assessment.
2. The fine-tuned dataset is from internet, and can poetntially generating inappropraiet contents or bias in the data.","C42, C91, C106, C120, C128",brief mention,brief mention,,
Salesforce/blip2-opt-2.7b,"1. This model uses one of the language model as its language model component, and therefore inherits the same risk and limitation of potentially generating harmful content, and cannot be deployed into applications without safety and fairness assessment.
2. The fine-tuned dataset is from internet, and can poetntially generating inappropraiet contents or bias in the data.","C42, C91, C106, C120, C128",brief mention,brief mention,,
Salesforce/codet5p-110m-embedding,false positive,,,,,
Salesforce/ctrl,"1. Make reference to literatures about bias and fairness.
2. The model can be misused to cause harm to individuals, organisations and other entities. Therefore, they evaluated the model themself, and by third parties before releasing the model.
3. They removed undesirable sources from the training dataset, and by probing the generation, they tuned the model to limit the misuse.
4. The intend purpose for this model is for researchers to guard against malicious use.
5. Recommend to monitor and detect the use, further screen input and output of the model of negative inputs.
6. Use this model to generate harmful things to humans is considered out-of-scope.","C4, C7, C21, C22, C26, C57, C65, C77, C127",,,,actionable
Salesforce/safety-flan-t5-base,false positive,,,,,
sasha/regardv3,false positive,,,,,
savasy/bert-base-turkish-sentiment-cased,false positive,,,,,
sazyou-roukaku/BracingEvoMix,false positive,,,,,
sebastian-hofstaetter/colbert-distilbert-margin_mse-T2-msmarco,1. The model inherits social bias from the base model as well as the training dataset.,"C42, C125",brief mention,,,
segmind/Segmind-Vega,"1. This model intends to provide researchers to probe its limitations and bias.
2. This model offers a safe and controlled way to generate content, reducing the risk of harmful or inappropraite outputs.
3. This model is not intende to create accurate representation of people or real-world information, and not suitable for high accuracy tasks.
4. The model is exposed to a diverse dataset, representing their efforts towards enabling more equitable models, but still can generate biased content.","C11, C21, C106, C127",brief mention,,,
segmind/SSD-1B,"1. This model intends to provide researchers to probe its limitations and bias.
2. This model offers a safe and controlled way to generate content, reducing the risk of harmful or inappropraite outputs.
3. This model is not intende to create accurate representation of people or real-world information, and not suitable for high accuracy tasks.
4. The model is exposed to a diverse dataset, representing their efforts towards enabling more equitable models, but still can generate biased content.","C11, C21, C106, C127",brief mention,,,
skt/ko-gpt-trinity-1.2B-v0.5,"1. The training dataset contain inappropriate contents, and therefore could generate inappropriate texts.
2. The model is not going to perform as well in other languages or dialets of Korean.
3. Limited transparency on model prediction and interpretion.","C66, C103, C122, C124",brief mention,,,
soleimanian/financial-roberta-large-sentiment,false positive,,,,,
speechbrain/mtl-mimic-voicebank,false positive,,,,,
speechbrain/spkrec-ecapa-voxceleb,false positive,,,,,
stabilityai/japanese-stablelm-base-alpha-7b,"1. The training data after applied with cleansing filters, still contain inappropraite content, and therefore recommend users to exercise with caution when using the model.
2. Use the model for cause harm to people is not recommended.","C5, C8, C121, C124",,,,
stabilityai/sd-turbo,"1. This model intends to provide researchers to probe its limitations and bias.
2. Safe model deployment potentilly can still generate harmful content.
3. Not recommend to generate factual, or true representation of real life even or people, due to the model is not trained on such data.
4. The model should not be used to violate stablility AI acceptable use policy.","C11, C21, C33, C120",,brief mention,,
stabilityai/stable-diffusion-x4-upscaler,"1. This model intends to provide researchers to probe its limitations and bias. 
2. Safe model deployment potentilly can still generate harmful content.
3. Not recommend to generate factual, or true representation of real life even or people, due to the model is not trained on such data.
4. It is considered model misuse to generate harmful content to people, or violation licnese and copyrights.
5. The training dataset contains inappropriate contents, and the authors have filtered the dataset to mitigate the issue.
6. The model output represent western cultures, and the performance for other models degrade, which will exacerbate biases.
7. List the environmental impact of the model with carbon emission for training the model.","C5, C11, C21, C33, C35, C70, C75, C103, C120, C122, C123",brief mention,brief mention,,
stabilityai/stable-diffusion-xl-refiner-1.0,"1. This model intends to provide researchers to probe its limitations and bias. 
2. Safe model deployment potentilly can still generate harmful content.
3. Not recommend to generate factual, or true representation of real life even or people, due to the model is not trained on such data.
4. The model generation could reinforce biases.","C11, C21, C106, C120",brief mention,brief mention,,
stabilityai/stable-video-diffusion-img2vid-xt,"1. This model intends to provide researchers to probe its limitations and bias. 
2. Safe model deployment potentilly can still generate harmful content.
3. Not recommend to generate factual, or true representation of real life even or people, due to the model is not trained on such data.
4. The model should not be used to violate stablility AI acceptable use policy.","C11, C21, C33, C120",,brief mention,,
stabilityai/StableBeluga-13B,"1. Before deployment, developers should perform safety test and tuning for their specific use, as it cannot cover all test case scenarios, and output canbe inaccurate or biased.","C106, C128, C129",brief mention,,brief mention,
stabilityai/stablelm-3b-4e1t,"1. Need to provide link to the license when using the model, and cannot suggest authors endorse users' use purpose.
2. The model can have unsafe or undesirable behaviour, therefore, must be corrected via evaluation and fine-tunening before deployment
3. The training dataset, even after cleasning filters, still contain inappropriate contents, and can reflect to the model output. Therefore, recommend users to exercise with caution in production systems.
4. Do not use this model for applications to cause harm to others.","C5, C8, C41, C70, C119, C120, C128, C129",,brief mention,,
stabilityai/stablelm-zephyr-3b,"1. Authors made efforts for making this model ethical compliance. They assess the harmful input, and red team in areas of self-harm methods, misinformation and hate speech. These findings were incorporated into the model release.
2. The model inherent biases and limitations in other LLM models.
3. The model is not trained against adversarial inputs, and suggest to pari with input and output classifiers to prevent harmful responses.
4. Need safe performance evaluation before apply to downstream applications.
5. Use the model to harm people is out-of-scope use.","C22, C42, C65, C70, C128",,,,actionable
StanfordAIMI/stanford-deidentifier-base,false positive,,,,,
suno/bark,"1. They wish their model to be used to improve accessibility tools in different languages.
2. Malicious use of this model is possible, such as voice clone. To mitigate this misuse, they also release classifier which can detect generated audio.","C87, C88, C89",,,,
T-Systems-onsite/cross-en-de-roberta-sentence-transformer,false positive,,,,,
t5-small,"1. Bias, risks, and limitations section, out-of-scope use section, recommendation section, and environmental impact section left empty.",C26,,,,
TalTechNLP/voxlingua107-epaca-tdnn,"1. The model is trained on a dataset, which inherent limitations and biases from it. This includes less accurate for smaller languages, works worse for female speech, foreign accent, and children's peech, and person with speech disorders.","C103, C104, C110, C125",brief mention,,,
teknium/OpenHermes-2-Mistral-7B,false positive,,,,,
TheBloke/CodeLlama-34B-fp16,"1. Use this model that violate laws, regulations or license is considered out-of-use, so is using languages other than English.
2. List the estimated carbon emission when training this model, and state that the emission is offset by Meta's sustainability program.
3. Make reference for safety evaluation to the research paper.
4. Before deployment, developers should perform safety test and tuning for their specific use, as it cannot cover all test case scenarios, and output canbe inaccurate or biased.
5. Make reference to external responsible use guide.","C33, C35, C36, C128, C129",,,,
TheBloke/deepseek-coder-33B-instruct-AWQ,false positive,,,,,
TheBloke/EverythingLM-13B-V3-16K-GPTQ,1. The model is uncensored.,C62,,,,
TheBloke/Llama-2-13B-fp16,"1. This model is fine-tuned using RLHF to align with human preference for helpfulness and safety.
2. Use this model that violate laws, regulations or license is considered out-of-use, so is using languages other than English.
3. List the estimated carbon emission when training this model, and state that the emission is offset by Meta's sustainability program.
4. Provide evaluation result for the pretrained models on automatic safety benchemark.
5. Before deployment, developers should perform safety test and tuning for their specific use, as it cannot cover all test case scenarios, and output canbe inaccurate or biased.
6. Make reference to external responsible use guide.","C1, C4, C33, C35, C36, C106, C120, C128, C129",brief mention,evaluated result,evaluated result,
TheBloke/Llama-2-70B-Chat-GPTQ,"1. This model is fine-tuned using RLHF to align with human preference for helpfulness and safety.
2. Use this model that violate laws, regulations or license is considered out-of-use, so is using languages other than English.
3. List the estimated carbon emission when training this model, and state that the emission is offset by Meta's sustainability program.
4. Provide evaluation result for the pretrained models on automatic safety benchemark.
5. Before deployment, developers should perform safety test and tuning for their specific use, as it cannot cover all test case scenarios, and output canbe inaccurate or biased.
6. Make reference to external responsible use guide.","C1, C4, C33, C35, C36, C106, C120, C128, C129",brief mention,evaluated result,brief mention,
TheBloke/LLaMA2-13B-Tiefighter-AWQ,"1. In some cases, there can be data leakage problem, and could mitigate by using > as a prefix. However, this can result in stronger fiction bias.
2. This model focus is on fiction, so the provided information can be not factually accurate.","C1, C22, C32",,,brief mention,actionable
TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ,1. They expect to develop ways to make the model repect guardrails in the future.,C69,,,,
TheBloke/orca_mini_v3_13B-GPTQ,"1. This model should follow llama license, and provide no gurantees.
2. The model could produce inaccurate results. Despite refining the pretraining data, the model coudl still generate inappropriate and biased content.
3. Recommend users to exercise caution and check information.","C1, C5, C8, C106, C119, C120",brief mention,brief mention,brief mention,
TheBloke/Synthia-70B-v1.2b-AWQ,false positive,,,,,
TheBloke/Upstage-Llama-2-70B-instruct-v2-AWQ,false positive,,,,,
TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ,"1. This model is uncensored, and has no guradrails.
2. Users are responsible for anything they interact with the model.",,,,,
TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ,"1. This model is uncensored, and has no guradrails.
2. Users are responsible for anything they interact with the model.",,,,,
TheBloke/zephyr-7B-alpha-GPTQ,"1. The model is not aligned with human preferences for safety with techniques like RLHF, therefore, the output can be problematic.",C59,,,,
thibaud/controlnet-sd21,"1. It is considered model misuse to harm people using this model. This includes generating inappropriate images to harm people, or contents that propagates stereotypes.",C70,,,,
THUDM/chatglm3-6b,"1. The model weights are free to use for research, and for commercial use after finishing questionnaire, under circumstance of not violating the license (which provides an external link).","C24, C41",,,,
THUDM/cogvlm-chat-hf,"1. To use the model weights, users need to comply with model license (with external link)","C24, C41",,,,
tiiuae/falcon-180B,"1. This model can be used for commercial purpose.
2. Production use need to go through risk assessment and mitigation, while use the model for harmful purpose is considered out of scope.
3. This model is mostly trained on a few languages, and will not generalise to other language. 
4. The training dataset is representative of the web, will carry stereotypes and biasees encountered online.","C70, C91, C125, C129, C130",,,,
tiiuae/falcon-40b,"1. This model can be used for commercial purpose without any restrictions.
2. Production use need to go through risk assessment and mitigation, while use the model for harmful purpose is considered out of scope.
3. This model is mostly trained on a few languages, and will not generalise to other language. 
4. The training dataset is representative of the web, will carry stereotypes and biasees encountered online.","C70, C91, C125, C129, C130",,,,
tiiuae/falcon-rw-1b,"1. Production use need to go through risk assessment and mitigation, while use the model for harmful purpose is considered out of scope.
2. This model is mostly trained on a few languages, and will not generalise to other language. 
3. The training dataset is representative of the web, will carry stereotypes and biasees encountered online.","C70, C91, C125, C129, C130",,,,
timm/convit_base.fb_in1k,false positive,,,,,
timm/swinv2_cr_tiny_ns_224.sw_in1k,false positive,,,,,
timm/vit_large_patch14_clip_224.openai_ft_in12k_in1k,false positive,,,,,
Titeiiko/OTIS-Official-Spam-Model,1. The intend use of this model is to mitigate and combat the unwanted and malicious content on digital communication channels.,C91,,,,
tner/roberta-large-ontonotes5,false positive,,,,,
togethercomputer/GPT-JT-6B-v1,false positive,,,,,
tomh/toxigen_roberta,false positive,,,,,
transfo-xl-wt103,"1. Not recommend to generate factual, or true representation of real life even or people, due to the model is not trained on such data.
2. Make reference to literatures for bias and fairness issues.","C7, C11",,,,
ufal/robeczech-base,"1.  Make reference to literatures for bias and fairness issues.
2. The model prediction can contain harmful and stereotypical contents, therefore recommend the users to be made ware of the risks, biases and limitations of the model.","C7, C8, C9, C106",brief mention,,,
UFNLP/gatortron-base,"1. The authors applied de-identification method for sensitive protected health information clinical text, and provide the way they did it as well as the reference system.",C5,this is interesting,,,
unitary/toxic-bert,"1. This model will classify sentences as toxic if words containing swearing, insults and other, which could be bias towards some minority groups.
2. The intended use of this model is to aird flagging out harmfu content quicker.
3. Provide literatures to biases in this particular context: hate speech detection.","C7, C11, C91",,,,
vilsonrodrigues/falcon-7b-instruct-sharded,"1. Harmful use of the model is considered out-of-scope use, recommend to develop guardrails, assessment and mitigation before applying the model to production use.
2. The training dataset is representative of the web, will carry stereotypes and biases encountered online.","C91, C125, C130",,,,
vinai/PhoGPT-7B5-Instruct,"1. The model can generate harmful, biased or factually incorrect content,  answer unsafe questions. Therefore recommend users to interact with caution.","C1, C9, C46, C106, C118",brief mention,,,
vinid/plip,"1. The model can be used by researchers to understand robustness, and biases factors of machine learning models.
2. Does not recommend any deployment use cases
3. Not recommended for non-deployed use cases such as image search, unless perform fixed class taxonomy in-domain test, due to varied performance of the model, and its potential harm.
4. This model is not trained or evaluated in languages other than English, therefore, it should only be used in English.
5. The data is in compliance with Twitter policy for data usage and sharing, and also detailed how they follow the policy to enable privacy concern for the data. 
6. The results does not constitute medical advice, and should be use as own risk, and follow the laws regulations and ethical considerations.
7. The authors do not guarantee anything for the model and disclaim liability from the model use.",C21,,,,
w11wo/indonesian-roberta-base-sentiment-classifier,1. The biases in the pretrained model as well as the dataset can be carried over to the result of this model.,"C42, C106, C125",brief mention,,,
WarriorMama777/OrangeMixs,"1. Cannot use the model to produce illegal or harmful content.
2. Authors claim no rights to the model outputs, and users are accountable for the model use, requiring them not to violate license.
3. If redistribute the weights of the model, same license need to be attached.","C19, C33, C41, C70",,,,
WizardLM/WizardLM-70B-V1.0,false positive,,,,,
xlm-mlm-en-2048,"1. Use this model to harm people is considered out-of-scope use.
2. Make reference to literatures for ethical considerations. This model can also generate harmful content, and users should be made aware of them.
3. Carbon emission left empty entry","C7, C8, C9, C26, C70",,,,
yahma/llama-7b-hf,"1. The model is not trained with human feedback, thus could generate offensive content or unhelpful answers.
2. There are 20 languages, but the training dataset contains majorly English, and thus the model is better performing in English than other languages. This also applies to different dialectrs.
3. The dataset is from web, and expect it will reflect biases in the source. And they evaluate the biases for different perspectives, as well as the toxicity of the model generations.
4. the model is not intended to inform decisions to human.
5. To mitigate the issue, the authors filtered the data from the web using several techniques.
6. The model should not be applied for downstream applicatrions without investigation and mitigation of the risks.","C5, C25, C65, C106, C112, C118, C128, C130",brief mention,,,
yiyanghkust/finbert-esg,false positive,,,,,
yiyanghkust/finbert-esg-9-categories,false positive,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
,,,,,,
